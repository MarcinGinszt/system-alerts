# PROMETHEUS RULES
# DO NOT REMOVE line above, used in `pre-commit` hook

groups:
  - name: WorkloadMissingReplicas
    rules:
      - alert: DeploymentMissingReplicas
        expr: kube_deployment_status_replicas_available{namespace=~"$NS"} != kube_deployment_status_replicas{namespace=~"$NS"}
        for: 15m
        annotations:
          summary: "Deployment {{$labels.namespace}}/{{$labels.deployment}} has missing replicas for 15m"
          impact: "Workload may be unavailable or have lost high availability"
          action: "Check why some replicas are not healthy"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe deployment {{ $labels.deployment }}"
      - alert: StatefulsetMissingReplicas
        expr: kube_statefulset_status_replicas_ready{namespace=~"$NS"} != kube_statefulset_status_replicas{namespace=~"$NS"}
        for: 15m
        annotations:
          summary: "Statefulset {{$labels.namespace}}/{{$labels.statefulset}} has missing replicas for 15m"
          impact: "Workload may be unavailable or have lost high availability"
          action: "Check why some replicas are not healthy"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe statefulset {{ $labels.statefulset }}"
      - alert: DaemonsetMissingReplicas
        expr: kube_daemonset_status_number_ready{namespace=~"$NS"} != kube_daemonset_status_desired_number_scheduled{namespace=~"$NS"}
        for: 15m
        annotations:
          summary: "Daemonset {{$labels.namespace}}/{{$labels.daemonset}} has missing replicas for 15m"
          impact: "Workload unavailable on some nodes"
          action: "Check why some replicas are not healthy"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe daemonset {{ $labels.daemonset }}"
      - alert: DeploymentMissingAllReplicas
        expr: kube_deployment_status_replicas_available{namespace=~"$NS"} == 0 and kube_deployment_status_replicas{namespace=~"$NS"} != 0
        for: 5m
        annotations:
          summary: "Deployment {{$labels.namespace}}/{{$labels.deployment}} has been missing all of its replicas for 5 minutes."
          impact: "Workload is down"
          action: "Check why all replicas are missing"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe deployment {{ $labels.deployment }}"
      - alert: StatefulsetMissingAllReplicas
        expr: kube_statefulset_status_replicas_ready{namespace=~"$NS"} == 0 and kube_statefulset_status_replicas{namespace=~"$NS"} != 0
        for: 5m
        annotations:
          summary: "Statefulset {{$labels.namespace}}/{{$labels.statefulset}} has been missing all of its replicas for 5 minutes."
          impact: "Workload is down"
          action: "Check why all replicas are missing"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe statefulset {{ $labels.statefulset }}"
      - alert: DaemonsetMissingAllReplicas
        expr: kube_daemonset_status_number_ready{namespace=~"$NS"} == 0 and kube_daemonset_status_desired_number_scheduled{namespace=~"$NS"} != 0
        for: 5m
        annotations:
          summary: "Daemonset {{$labels.namespace}}/{{$labels.daemonset}} has been missing all of its replicas for 5 minutes."
          impact: "Workload is down"
          action: "Check why all replicas are missing"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe daemonset {{ $labels.daemonset }}"
  - name: ContainerIssues
    rules:
      - alert: ContainerRestartingOften
        expr: increase(kube_pod_container_status_restarts_total{namespace=~"$NS"}[10m]) > 3
        annotations:
          summary: "Container {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} has restarted more than 3 times in the last 10m"
          impact: "Container may be crashlooping and not working as expected"
          action: "Check pod status and container logs to figure out if there's a problem"
          command: "kubectl --context $ENVIRONMENT-$PROVIDER --namespace {{ $labels.namespace }} describe pod {{ $labels.pod }}"
          logs: "https://grafana.$ENVIRONMENT.aws.uw.systems/explore?orgId=1&left=%7B%22datasource%22%3A%22P8E80F9AEF21F6940%22%2C%22queries%22%3A%5B%7B%22refId%22%3A%22A%22%2C%22expr%22%3A%22%7Bkubernetes_cluster%3D%5C%22$ENVIRONMENT-$PROVIDER%5C%22%2C+kubernetes_namespace%3D%5C%22{{ $labels.namespace }}%5C%22%2C+app_kubernetes_io_name%3D%5C%{{ $labels.label_app_kubernetes_io_name }}%5C%22%7D%22%2C%22queryType%22%3A%22range%22%2C%22datasource%22%3A%7B%22type%22%3A%22loki%22%2C%22uid%22%3A%22P8E80F9AEF21F6940%22%7D%2C%22editorMode%22%3A%22code%22%7D%5D%2C%22range%22%3A%7B%22from%22%3A%22now-1h%22%2C%22to%22%3A%22now%22%7D%7D"
      - alert: ContainerCpuThrottled
        # https://github.com/kubernetes-monitoring/kubernetes-mixin/issues/108#issuecomment-432796867
        expr: sum(increase(container_cpu_cfs_throttled_periods_total{namespace=~"$NS"}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{namespace=~"$NS"}[5m])) by (container, pod, namespace) > 0.95
        for: 15m
        annotations:
          summary: "Container {{$labels.namespace}}/{{$labels.pod}}/{{$labels.container}} is being CPU throttled."
          impact: "Container might take longer than normal to respond to requests."
          action: "Investigate CPU consumption and adjust pods resources if needed."
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/VAE0wIcik/kubernetes-pod-resources?orgId=1&refresh=1m&from=now-12h&to=now&var-instance=All&var-namespace={{ $labels.namespace }}"
  - name: StorageIssues
    rules:
      - alert: DiskFillingUpin72h
        expr: predict_linear(kubelet_volume_stats_available_bytes{namespace=~"$NS"}[1h], 72 * 3600) < 0 and kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes{namespace=~"$NS"} > 0.2
        for: 5m
        annotations:
          summary: "Volume {{$labels.namespace}}/{{$labels.persistentvolumeclaim}} will fill up in 72h"
          impact: "Exhausting available disk space will most likely result in service disruption"
          action: "Investigate disk usage and adjust volume size if necessary."
          dashboard: "https://grafana.$ENVIRONMENT.$PROVIDER.uw.systems/d/919b92a8e8041bd567af9edab12c840c/kubernetes-persistent-volumes?orgId=1&refresh=10s&var-datasource=default&var-cluster=&var-namespace={{ $labels.namespace }}&var-volume={{ $labels.persistentvolumeclaim }}"
  - name: ObservabilityIssues
    rules:
      - alert: WorkloadDroppingLogs
        expr: rate(logentry_dropped_lines_by_label_total{label_name="limit_key", label_value=~"$NS"}[5m]) > 10
        for: 10m
        annotations:
          summary: "{{ $labels.label_value }} is being noisy and dropping logs"
          impact: "Logs won't be available in Loki"
          action: "Reduce verbosity of the workload"
